{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOMEWORK OPTIMIZATION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Matrix generation & data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of AX: (1000, 50)\n",
      "b: [37 24 26 45 42 38 24 37 17 43 20 30 46 39 29 43 48 24 26 42 17 39 13 44\n",
      " 33 21 36 25 42 19 13  0 29 37  7 19  5 17 29  5  4 18 48 18 13 36 13 44\n",
      "  6  7 16 26 40 20  1 24 39 18  8 19  8  5 39  7 29 38 15 46  3 20 39 41\n",
      " 19 37 37  8 23 10  0  2 34 49 47 38 23 42 25  7 26  6  7 27 32 13 34  7\n",
      " 37 45 30 31 32 20 38  8 47  9 44 26 30 34 14 48 27 18 16 11 42 22 23 39\n",
      " 26 24 13  8 44 45 15 24 32 40 19 20  5 33 35 15 12 18  9 35 37 31  7 32\n",
      " 10 20 25 25 47 11  6 49 35 17 42 10 41 24 30 32 38 20 27  6  8 23  5 43\n",
      "  8 48 22 33 13  9 41  6 30 41 38 10 11 47 22 43 21 46 14  8 44 26  8 38\n",
      "  3 43 35  0 18 14 35  3 41  5  1 17  1 10  9 13 37  1 44 47 19 24 19 17\n",
      "  5 44 31  7 28  4 48  9 46  5 14 21 48 10 18 11 33 11 18 14 17 42  3 29\n",
      "  0 25 34 23 11 10  0 15 13 35  2 34 41 43 49 43 43 31 29 40 29 13 13 23\n",
      "  9 17 29 28  8 36 43 30 12 16  7 27 23  7 47 12  5 34 38 31 25 14 39 11\n",
      "  1 14 40 39 49 44  9 19 18 33  4 43 10  0 34 18 10 33  4 31 23 37 12  7\n",
      " 46 40 22 20  9  9 24 18 20  8 23  1 39 46  1 33 46  0 34 48 24 38 42 20\n",
      " 43 14 32 19 25 44 19 26 26 29  0  1 39 46 14 35 32 23  7  1 10 37 14 46\n",
      " 11 28  8 43 29 42  3 30 25 42 43 25 12 15  2 39 26  0 31 40 15 47 13  1\n",
      "  7 15 44 22 45 37  7 23 49 31 22 48 37  4 41 12  9 12 49 13 35 22 44 10\n",
      "  3 12 11 17 46 37 25 24 16 21 37  6 33  7  1 20 17 22  9  5 44 26  5 48\n",
      " 12 11 33 40 49  8 48 39 46 25 25 32 46 47 49 23 23  7 29 32  3 10 42 30\n",
      " 42  0 37 26 44 39 11 30  4 46 16  5 12 49  7 38 39 47  0 41 18 25 25 44\n",
      " 39 16 31 40 33 39 15 38 26 48  9 18 36 22 26 36 42 11 24 23 43 34  6 19\n",
      "  5  4 22 11  1  3 40 17 49 14 13 21 20 34 21 42 39 21 18 22 40 44 20 10\n",
      "  4 37  7  5 38 42 34 40  0 20 18 11 32  4 22 11 30 49 26  7 40 39 32 29\n",
      " 31 26 10  3  7 15 39 28 49 31 23 23 23 42 49 18 29 38 48  0 49  1  8 17\n",
      " 22 42 45  6 10 49  4  0  5 48 22 15 38 21 17 38 19  7 34 31 33 36 17 36\n",
      " 28 37 24 17 45 30 39  8 36 25 10  7 22 46 47 22 37 18  0 47 30 40 29 14\n",
      " 40  2 33 32 48 47 22  0  8 21  3 35 17 12 43 12  6 36 12 14 11  5 28  4\n",
      " 21  1 21  7 23 12 10 12 25 30 46 15 45 39 20  2  7  7 33 46  2 38 44 42\n",
      " 12  7 35  2 35 42 34 22 20 42 19 27 16 35 25 38  4 20 49 39 16 31  9 49\n",
      " 42 39  8  4 40 39 37 41 37  3 27 40 48 18 10 42  9 48 22 41 11 23 27 25\n",
      " 30  3 42  7  1  9 32 10  2 15 40 20 29  9 12 11 36 29 15 27 26 49 27 22\n",
      " 13  1 16 21 29 40 35 21 49  9 29 24 11 43 14 39 15 36 35 21 41 31 26  4\n",
      " 12  6 46 14  7 48  2 48  2 17 20 12 36 43 24 25 31 17 24 35 21 37  6 23\n",
      "  9 20 48  7 15 38  4 41 39 37 25 12 45  4 34 28 30 16 26 42 27 39 38  0\n",
      " 32  6 49 49 39 46 42 15 13 32 19 44 27 20  9 39 12 24 23 42 12 47 40  4\n",
      " 40 36  4 27 18  7 22 36 43 47 29 38  8 21  7 17 30 15 41 27 22 26 31 18\n",
      " 32 35 18 11 16 32 47 49 31 46 42 22 36 13 39 37  7  7 16 36 32 37 37  8\n",
      " 29 34 49  9 45  0  9 25 38 17 37 35 32 15 41 14 28 41 10 39 25 31 27 25\n",
      " 17 23 41  8 41 11 18 15 29 12 19 23  0  1 19  5 35 41 11 16 30 17 23 47\n",
      " 22  9 22 35 11 37 40 46 47 45 23  2 31  0 41 32 26 35 15  3 42 12 38 27\n",
      " 39 21 29 26 34 41  4 44 17 25 14 18 19 11  4 23 15  7 25 13 48 42 20 22\n",
      " 25 17 29 24 48 29 49 47  0 21 32 49  9 42 10 16]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.datasets import cifar10\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from joblib import Parallel, delayed\n",
    "import tensorflow as tf\n",
    "\n",
    "d = 1000  # Number of features\n",
    "K = 50  # Number of classes\n",
    "m = 1000   # Number of observations\n",
    "\n",
    "# Sampling A, X, E from a normal distribution N(0,1)\n",
    "A = np.random.normal(0, 1, (m, d))  # data matrix of 1000x1000\n",
    "X = np.random.normal(0, 1, (d, K))  # parameters\n",
    "E = np.random.normal(0, 1, (m, K))  # Noise\n",
    "\n",
    "# Computing AX + E\n",
    "AX = A.dot(X) + E\n",
    "\n",
    "# Checking the shape of the result for verification\n",
    "print(\"Shape of AX:\", AX.shape)\n",
    "\n",
    "# Finding the indices of the maximum values along each column (axis 0)\n",
    "b = np.argmax(AX, axis=1)\n",
    "\n",
    "# Printing b to verify correct computation\n",
    "print(\"b:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione softmax\n",
    "def softmax(scores):\n",
    "    exp_scores = np.exp(scores - np.max(scores, axis=0, keepdims=True))\n",
    "    return exp_scores / np.sum(exp_scores, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 / 50\n",
      "iter 1 / 50\n",
      "iter 2 / 50\n",
      "iter 3 / 50\n",
      "iter 4 / 50\n",
      "iter 5 / 50\n",
      "iter 6 / 50\n",
      "iter 7 / 50\n",
      "iter 8 / 50\n",
      "iter 9 / 50\n",
      "iter 10 / 50\n",
      "iter 11 / 50\n",
      "iter 12 / 50\n",
      "iter 13 / 50\n",
      "iter 14 / 50\n",
      "iter 15 / 50\n",
      "iter 16 / 50\n",
      "iter 17 / 50\n",
      "iter 18 / 50\n",
      "iter 19 / 50\n",
      "iter 20 / 50\n",
      "iter 21 / 50\n",
      "iter 22 / 50\n",
      "iter 23 / 50\n",
      "iter 24 / 50\n",
      "iter 25 / 50\n",
      "iter 26 / 50\n",
      "iter 27 / 50\n",
      "iter 28 / 50\n",
      "iter 29 / 50\n",
      "iter 30 / 50\n",
      "iter 31 / 50\n",
      "iter 32 / 50\n",
      "iter 33 / 50\n",
      "iter 34 / 50\n",
      "iter 35 / 50\n",
      "iter 36 / 50\n",
      "iter 37 / 50\n",
      "iter 38 / 50\n",
      "iter 39 / 50\n",
      "iter 40 / 50\n",
      "iter 41 / 50\n",
      "iter 42 / 50\n",
      "iter 43 / 50\n",
      "iter 44 / 50\n",
      "iter 45 / 50\n",
      "iter 46 / 50\n",
      "iter 47 / 50\n",
      "iter 48 / 50\n",
      "iter 49 / 50\n",
      "Final weights:\n",
      "[[ 0.36986028  0.28210697  1.02227137 ...  0.93843555  0.28499279\n",
      "   0.12219891]\n",
      " [ 1.9500942   0.57037313 -1.15149732 ...  0.32848853  2.35509575\n",
      "  -0.41692205]\n",
      " [ 0.4112971  -1.00922258 -0.26876957 ... -0.02846488 -0.57209187\n",
      "   1.04543898]\n",
      " ...\n",
      " [-0.48513051 -0.13087096 -0.35948492 ...  1.57618863  0.46179894\n",
      "   0.04404178]\n",
      " [-0.21474549  0.77244787  0.93431773 ... -1.84555264 -0.39778063\n",
      "   0.52913549]\n",
      " [-0.32990368  0.23369221  0.15076957 ... -0.20603145  0.45894303\n",
      "  -1.90781396]]\n"
     ]
    }
   ],
   "source": [
    "# Inizializzazione dei pesi\n",
    "weights = np.random.normal(0, 1, (m, d))\n",
    "\n",
    "# Tasso di apprendimento e numero di iterazioni\n",
    "eta = 0.02\n",
    "num_iterations = 50\n",
    "\n",
    "# Ciclo di ottimizzazione per la discesa del gradiente\n",
    "for _ in range(num_iterations):\n",
    "    scores = weights.dot(X)  # m x K\n",
    "    probs = softmax(scores)\n",
    "\n",
    "    # Calcolo del gradiente\n",
    "    grad = np.zeros_like(weights)\n",
    "    for k in range(m):\n",
    "        for i in range(K):\n",
    "            grad[k] += (probs[k, i] - (b[i] == k)) * X[:, i]\n",
    "    \n",
    "    # Aggiornamento dei pesi\n",
    "    weights -= eta * grad\n",
    "    print('iter', _, '/', num_iterations)\n",
    "\n",
    "# Stampa dei pesi finali\n",
    "print(\"Final weights:\")\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 / 50\n",
      "iter 1 / 50\n",
      "iter 2 / 50\n",
      "iter 3 / 50\n",
      "iter 4 / 50\n",
      "iter 5 / 50\n",
      "iter 6 / 50\n",
      "iter 7 / 50\n",
      "iter 8 / 50\n",
      "iter 9 / 50\n",
      "iter 10 / 50\n",
      "iter 11 / 50\n",
      "iter 12 / 50\n",
      "iter 13 / 50\n",
      "iter 14 / 50\n",
      "iter 15 / 50\n",
      "iter 16 / 50\n",
      "iter 17 / 50\n",
      "iter 18 / 50\n",
      "iter 19 / 50\n",
      "iter 20 / 50\n",
      "iter 21 / 50\n",
      "iter 22 / 50\n",
      "iter 23 / 50\n",
      "iter 24 / 50\n",
      "iter 25 / 50\n",
      "iter 26 / 50\n",
      "iter 27 / 50\n",
      "iter 28 / 50\n",
      "iter 29 / 50\n",
      "iter 30 / 50\n",
      "iter 31 / 50\n",
      "iter 32 / 50\n",
      "iter 33 / 50\n",
      "iter 34 / 50\n",
      "iter 35 / 50\n",
      "iter 36 / 50\n",
      "iter 37 / 50\n",
      "iter 38 / 50\n",
      "iter 39 / 50\n",
      "iter 40 / 50\n",
      "iter 41 / 50\n",
      "iter 42 / 50\n",
      "iter 43 / 50\n",
      "iter 44 / 50\n",
      "iter 45 / 50\n",
      "iter 46 / 50\n",
      "iter 47 / 50\n",
      "iter 48 / 50\n",
      "iter 49 / 50\n",
      "Final weights:\n",
      "[[ 0.53825694 -1.14213837  1.01710882 ...  0.03763827 -0.51342047\n",
      "   1.27155045]\n",
      " [ 0.27041534 -0.30009229 -1.77385636 ...  1.79158046 -0.36295742\n",
      "   1.34812424]\n",
      " [-0.30815305 -0.86218854 -0.29331221 ...  0.71454063  0.34338403\n",
      "  -0.23913163]\n",
      " ...\n",
      " [ 0.50883925  0.82344316 -0.37920583 ...  0.90217782 -0.56782103\n",
      "   0.72192571]\n",
      " [-0.22378951 -1.1209033   1.25036735 ... -0.02023916  0.52573903\n",
      "   0.39966821]\n",
      " [ 2.16181645 -0.04878745  0.25944409 ... -1.25017458  1.5731683\n",
      "  -0.79073985]]\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.normal(0, 1, (m, d))\n",
    "\n",
    "# Tasso di apprendimento e numero di iterazioni\n",
    "eta = 0.02\n",
    "num_iterations = 50\n",
    "\n",
    "# Ciclo di ottimizzazione per BCGD con Gauss-Southwell\n",
    "for _ in range(num_iterations):\n",
    "    scores = weights.dot(X)  # m x K\n",
    "    probs = softmax(scores)\n",
    "\n",
    "    # Calcolo del gradiente completo\n",
    "    grad = np.zeros_like(weights)\n",
    "    for k in range(m):\n",
    "        for i in range(K):\n",
    "            grad[k] += (probs[k, i] - (b[i] == k)) * X[:, i]\n",
    "\n",
    "    # Selezionare il blocco con il gradiente massimo in valore assoluto\n",
    "    block_norms = np.linalg.norm(grad, axis=1)\n",
    "    block_to_update = np.argmax(block_norms)\n",
    "\n",
    "    # Aggiornamento solo del blocco selezionato\n",
    "    weights[block_to_update] -= eta * grad[block_to_update]\n",
    "    print('iter', _, '/', num_iterations)\n",
    "\n",
    "\n",
    "# Stampa dei pesi finali\n",
    "print(\"Final weights:\")\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WINE DATASET multiclass classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
      "0      1    14.23        1.71  2.43               15.6        127   \n",
      "1      1    13.20        1.78  2.14               11.2        100   \n",
      "2      1    13.16        2.36  2.67               18.6        101   \n",
      "3      1    14.37        1.95  2.50               16.8        113   \n",
      "4      1    13.24        2.59  2.87               21.0        118   \n",
      "\n",
      "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "0           2.80        3.06                  0.28             2.29   \n",
      "1           2.65        2.76                  0.26             1.28   \n",
      "2           2.80        3.24                  0.30             2.81   \n",
      "3           3.85        3.49                  0.24             2.18   \n",
      "4           2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
      "0             5.64  1.04                          3.92     1065  \n",
      "1             4.38  1.05                          3.40     1050  \n",
      "2             5.68  1.03                          3.17     1185  \n",
      "3             7.80  0.86                          3.45     1480  \n",
      "4             4.32  1.04                          2.93      735  \n"
     ]
    }
   ],
   "source": [
    "# URL for the Wine dataset from the UCI Machine Learning Repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "\n",
    "# Column names for the Wine dataset\n",
    "columns = [\n",
    "    \"Class\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "data = pd.read_csv(url, header=None, names=columns)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
      "0       2596      51      3                               258   \n",
      "1       2590      56      2                               212   \n",
      "2       2804     139      9                               268   \n",
      "3       2785     155     18                               242   \n",
      "4       2595      45      2                               153   \n",
      "\n",
      "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
      "0                               0                              510   \n",
      "1                              -6                              390   \n",
      "2                              65                             3180   \n",
      "3                             118                             3090   \n",
      "4                              -1                              391   \n",
      "\n",
      "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
      "0            221             232            148   \n",
      "1            220             235            151   \n",
      "2            234             238            135   \n",
      "3            238             238            122   \n",
      "4            220             234            150   \n",
      "\n",
      "   Horizontal_Distance_To_Fire_Points  ...  Soil_Type_31  Soil_Type_32  \\\n",
      "0                                6279  ...             0             0   \n",
      "1                                6225  ...             0             0   \n",
      "2                                6121  ...             0             0   \n",
      "3                                6211  ...             0             0   \n",
      "4                                6172  ...             0             0   \n",
      "\n",
      "   Soil_Type_33  Soil_Type_34  Soil_Type_35  Soil_Type_36  Soil_Type_37  \\\n",
      "0             0             0             0             0             0   \n",
      "1             0             0             0             0             0   \n",
      "2             0             0             0             0             0   \n",
      "3             0             0             0             0             0   \n",
      "4             0             0             0             0             0   \n",
      "\n",
      "   Soil_Type_38  Soil_Type_39  Cover_Type  \n",
      "0             0             0           5  \n",
      "1             0             0           5  \n",
      "2             0             0           2  \n",
      "3             0             0           2  \n",
      "4             0             0           5  \n",
      "\n",
      "[5 rows x 55 columns]\n",
      "Elevation                             0\n",
      "Aspect                                0\n",
      "Slope                                 0\n",
      "Horizontal_Distance_To_Hydrology      0\n",
      "Vertical_Distance_To_Hydrology        0\n",
      "Horizontal_Distance_To_Roadways       0\n",
      "Hillshade_9am                         0\n",
      "Hillshade_Noon                        0\n",
      "Hillshade_3pm                         0\n",
      "Horizontal_Distance_To_Fire_Points    0\n",
      "Wilderness_Area_0                     0\n",
      "Wilderness_Area_1                     0\n",
      "Wilderness_Area_2                     0\n",
      "Wilderness_Area_3                     0\n",
      "Soil_Type_0                           0\n",
      "Soil_Type_1                           0\n",
      "Soil_Type_2                           0\n",
      "Soil_Type_3                           0\n",
      "Soil_Type_4                           0\n",
      "Soil_Type_5                           0\n",
      "Soil_Type_6                           0\n",
      "Soil_Type_7                           0\n",
      "Soil_Type_8                           0\n",
      "Soil_Type_9                           0\n",
      "Soil_Type_10                          0\n",
      "Soil_Type_11                          0\n",
      "Soil_Type_12                          0\n",
      "Soil_Type_13                          0\n",
      "Soil_Type_14                          0\n",
      "Soil_Type_15                          0\n",
      "Soil_Type_16                          0\n",
      "Soil_Type_17                          0\n",
      "Soil_Type_18                          0\n",
      "Soil_Type_19                          0\n",
      "Soil_Type_20                          0\n",
      "Soil_Type_21                          0\n",
      "Soil_Type_22                          0\n",
      "Soil_Type_23                          0\n",
      "Soil_Type_24                          0\n",
      "Soil_Type_25                          0\n",
      "Soil_Type_26                          0\n",
      "Soil_Type_27                          0\n",
      "Soil_Type_28                          0\n",
      "Soil_Type_29                          0\n",
      "Soil_Type_30                          0\n",
      "Soil_Type_31                          0\n",
      "Soil_Type_32                          0\n",
      "Soil_Type_33                          0\n",
      "Soil_Type_34                          0\n",
      "Soil_Type_35                          0\n",
      "Soil_Type_36                          0\n",
      "Soil_Type_37                          0\n",
      "Soil_Type_38                          0\n",
      "Soil_Type_39                          0\n",
      "Cover_Type                            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL per il dataset Covertype dal UCI Machine Learning Repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
    "\n",
    "# Caricare il dataset in un DataFrame pandas\n",
    "columns = [\n",
    "    \"Elevation\", \"Aspect\", \"Slope\",\n",
    "    \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\", \"Hillshade_9am\", \"Hillshade_Noon\",\n",
    "    \"Hillshade_3pm\", \"Horizontal_Distance_To_Fire_Points\"\n",
    "] + [f\"Wilderness_Area_{i}\" for i in range(4)] + [f\"Soil_Type_{i}\" for i in range(40)] + [\"Cover_Type\"]\n",
    "\n",
    "data = pd.read_csv(url, header=None, names=columns)\n",
    "\n",
    "# Visualizzare le prime righe del DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Controllare la presenza di valori mancanti\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del dataset CIFAR-10...\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 12s 0us/step\n",
      "Dimensioni del training set: (50000, 32, 32, 3)\n",
      "Dimensioni del test set: (10000, 32, 32, 3)\n",
      "Dataset caricato in 14.27 secondi.\n",
      "Preprocessing del dataset...\n"
     ]
    }
   ],
   "source": [
    "# Caricamento del dataset CIFAR-10\n",
    "print(\"Caricamento del dataset CIFAR-10...\")\n",
    "(start_time, load_time) = (time.time(), 0)\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Visualizzare le dimensioni del dataset\n",
    "print(f'Dimensioni del training set: {X_train.shape}')\n",
    "print(f'Dimensioni del test set: {X_test.shape}')\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Dataset caricato in {load_time:.2f} secondi.\")\n",
    "\n",
    "# Preprocessing\n",
    "print(\"Preprocessing del dataset...\")\n",
    "(start_time, preprocessing_time) = (time.time(), 0)\n",
    "\n",
    "# Ridimensionare le immagini in vettori\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del dataset Digits...\n",
      "Dimensioni del training set: (1437, 64)\n",
      "Dimensioni del test set: (360, 64)\n",
      "Dataset caricato in 0.02 secondi.\n",
      "Preprocessing del dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Caricamento del dataset Digits...\")\n",
    "(start_time, load_time) = (time.time(), 0)\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Suddivisione del dataset in training set e test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualizzare le dimensioni del dataset\n",
    "print(f'Dimensioni del training set: {X_train.shape}')\n",
    "print(f'Dimensioni del test set: {X_test.shape}')\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Dataset caricato in {load_time:.2f} secondi.\")\n",
    "\n",
    "# Preprocessing\n",
    "print(\"Preprocessing del dataset...\")\n",
    "(start_time, preprocessing_time) = (time.time(), 0)\n",
    "\n",
    "# Normalizzare le immagini\n",
    "X_train = X_train.astype('float32') / 16\n",
    "X_test = X_test.astype('float32') / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.datasets import cifar10\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from joblib import Parallel, delayed\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dataset 20 Newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=None, shuffle=True, random_state=42)\n",
    "\n",
    "# Trasforma i testi in vettori TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  # Limitando le caratteristiche per migliorare le prestazioni\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "y = newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (11307, 1000)\n",
      "X_test shape: (7539, 1000)\n",
      "y_train shape: (11307,)\n",
      "y_test shape: (7539,)\n"
     ]
    }
   ],
   "source": [
    "# Dividi i dati in set di addestramento e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Stampa le dimensioni dei dati trasformati\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode le etichette\n",
    "y_train_one_hot = np.eye(np.max(y_train) + 1)[y_train]\n",
    "y_test_one_hot = np.eye(np.max(y_test) + 1)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizzare le caratteristiche\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False per dati sparsi\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (11307, 1000)\n",
      "X_val shape: (3769, 1000)\n",
      "X_test shape: (3770, 1000)\n",
      "y_train_one_hot shape: (11307, 20)\n",
      "y_val_one_hot shape: (3769, 20)\n",
      "y_test_one_hot shape: (3770, 20)\n"
     ]
    }
   ],
   "source": [
    "# Dividere il training set in training e validation set\n",
    "X_val, X_test, y_val_one_hot, y_test_one_hot = train_test_split(X_test, y_test_one_hot, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = sp.csr_matrix(X_train)\n",
    "y_train_one_hot = sp.csr_matrix(y_train_one_hot)\n",
    "X_val = sp.csr_matrix(X_val)\n",
    "y_val_one_hot = sp.csr_matrix(y_val_one_hot)\n",
    "X_test = sp.csr_matrix(X_test)\n",
    "y_test_one_hot = sp.csr_matrix(y_test_one_hot)\n",
    "\n",
    "# Stampa le dimensioni dei dati trasformati\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_train_one_hot shape: {y_train_one_hot.shape}')\n",
    "print(f'y_val_one_hot shape: {y_val_one_hot.shape}')\n",
    "print(f'y_test_one_hot shape: {y_test_one_hot.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def compute_cost(probs, y_sparse):\n",
    "    m = y_sparse.shape[0]\n",
    "    log_likelihood = -np.log(probs[np.arange(m), y_sparse.indices])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "def compute_accuracy(X, y_sparse, weights):\n",
    "    y_pred = predict(X, weights)\n",
    "    y_true = y_sparse.indices[y_sparse.indptr[:-1]]\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    return accuracy\n",
    "\n",
    "def gradient_descent(X, y, X_val, y_val, weights, learning_rate, num_epochs, batch_size):\n",
    "    m, n = X.shape\n",
    "    for epoch in range(num_epochs):\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, m, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            \n",
    "            logits = X_batch.dot(weights)\n",
    "            probs = softmax(logits)\n",
    "            gradient = X_batch.T.dot(probs - y_batch) / batch_indices.shape[0]\n",
    "            weights -= learning_rate * gradient\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            train_cost = compute_cost(softmax(X.dot(weights)), y)\n",
    "            val_cost = compute_cost(softmax(X_val.dot(weights)), y_val)\n",
    "            train_accuracy = compute_accuracy(X, y, weights)\n",
    "            val_accuracy = compute_accuracy(X_val, y_val, weights)\n",
    "            print(f'Epoch {epoch} completed')\n",
    "            print(f'Train Cost: {train_cost}, Train Accuracy: {train_accuracy}')\n",
    "            print(f'Validation Cost: {val_cost}, Validation Accuracy: {val_accuracy}')\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def block_coordinate_gradient_descent(X, y, X_val, y_val, weights, learning_rate, num_epochs, batch_size, block_size):\n",
    "    m, n = X.shape\n",
    "    n_blocks = n // block_size\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, m, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "\n",
    "            for block in range(n_blocks):\n",
    "                start = block * block_size\n",
    "                end = (block + 1) * block_size\n",
    "\n",
    "                X_block = X_batch[:, start:end]\n",
    "                W_block = weights[start:end]\n",
    "\n",
    "                logits = X_batch.dot(weights)\n",
    "                probs = softmax(logits)\n",
    "                \n",
    "                gradient = X_block.T.dot(probs - y_batch) / batch_indices.shape[0]\n",
    "                W_block -= learning_rate * gradient\n",
    "\n",
    "                weights[start:end] = W_block\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            train_cost = compute_cost(softmax(X.dot(weights)), y)\n",
    "            val_cost = compute_cost(softmax(X_val.dot(weights)), y_val)\n",
    "            train_accuracy = compute_accuracy(X, y, weights)\n",
    "            val_accuracy = compute_accuracy(X_val, y_val, weights)\n",
    "            print(f'Epoch {epoch} completed')\n",
    "            print(f'Train Cost: {train_cost}, Train Accuracy: {train_accuracy}')\n",
    "            print(f'Validation Cost: {val_cost}, Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "\n",
    "    return weights\n",
    "\n",
    "def predict(X, weights):\n",
    "    logits = X.dot(weights)\n",
    "    return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "n_classes = y_train_one_hot.shape[1]\n",
    "learning_rate = 0.005\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "block_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed\n",
      "Train Cost: 1.9847935393026381, Train Accuracy: 0.7452020872026178\n",
      "Validation Cost: 2.0323528341533015, Validation Accuracy: 0.703369594056779\n",
      "Epoch 2 completed\n",
      "Train Cost: 1.237264455865634, Train Accuracy: 0.7866808171928894\n",
      "Validation Cost: 1.3374788852851505, Validation Accuracy: 0.7442292385248076\n",
      "Epoch 4 completed\n",
      "Train Cost: 0.9772284541207873, Train Accuracy: 0.8097638630936588\n",
      "Validation Cost: 1.1131920029469862, Validation Accuracy: 0.7529848766250995\n",
      "Epoch 6 completed\n",
      "Train Cost: 0.8404420108978072, Train Accuracy: 0.8267444945608915\n",
      "Validation Cost: 1.0042369620415548, Validation Accuracy: 0.7625364818254179\n",
      "Epoch 8 completed\n",
      "Train Cost: 0.7522400441555318, Train Accuracy: 0.8399221721057751\n",
      "Validation Cost: 0.9406807266840748, Validation Accuracy: 0.7665163173255506\n",
      "Epoch 10 completed\n",
      "Train Cost: 0.6886981079162874, Train Accuracy: 0.8512425930839304\n",
      "Validation Cost: 0.8984517395599237, Validation Accuracy: 0.7704961528256832\n",
      "Epoch 12 completed\n",
      "Train Cost: 0.6397930176395565, Train Accuracy: 0.859556027239763\n",
      "Validation Cost: 0.8688704460388802, Validation Accuracy: 0.7720880870257363\n",
      "Epoch 14 completed\n",
      "Train Cost: 0.6004823665523782, Train Accuracy: 0.8683116653400549\n",
      "Validation Cost: 0.8472142250441109, Validation Accuracy: 0.7699655080923322\n",
      "Epoch 16 completed\n",
      "Train Cost: 0.5678190211789893, Train Accuracy: 0.8748562837180508\n",
      "Validation Cost: 0.8306214515885221, Validation Accuracy: 0.7710267975590342\n",
      "Epoch 18 completed\n",
      "Train Cost: 0.5400466071247624, Train Accuracy: 0.8820199876182896\n",
      "Validation Cost: 0.8180869514805291, Validation Accuracy: 0.7739453435924648\n",
      "Epoch 20 completed\n",
      "Train Cost: 0.5160665384789119, Train Accuracy: 0.8885646059962855\n",
      "Validation Cost: 0.8078878848258982, Validation Accuracy: 0.7715574422923852\n",
      "Epoch 22 completed\n",
      "Train Cost: 0.4949650290155193, Train Accuracy: 0.894224816485363\n",
      "Validation Cost: 0.8002948085041419, Validation Accuracy: 0.7734146988591138\n",
      "Epoch 24 completed\n",
      "Train Cost: 0.476236183720512, Train Accuracy: 0.8999734677633324\n",
      "Validation Cost: 0.7943474934719803, Validation Accuracy: 0.7734146988591138\n",
      "Epoch 26 completed\n",
      "Train Cost: 0.45941640890867597, Train Accuracy: 0.9030688953745467\n",
      "Validation Cost: 0.7894048433946257, Validation Accuracy: 0.7734146988591138\n",
      "Epoch 28 completed\n",
      "Train Cost: 0.44419731146728897, Train Accuracy: 0.90563367825241\n",
      "Validation Cost: 0.7855248274888094, Validation Accuracy: 0.7731493764924383\n",
      "Epoch 30 completed\n",
      "Train Cost: 0.43033220088036434, Train Accuracy: 0.9103210400636774\n",
      "Validation Cost: 0.7824701294978859, Validation Accuracy: 0.7715574422923852\n",
      "Epoch 32 completed\n",
      "Train Cost: 0.41760901292402136, Train Accuracy: 0.9135049084637835\n",
      "Validation Cost: 0.7800733190994685, Validation Accuracy: 0.7715574422923852\n",
      "Epoch 34 completed\n",
      "Train Cost: 0.40588608585934766, Train Accuracy: 0.9166003360749978\n",
      "Validation Cost: 0.7785599771523201, Validation Accuracy: 0.7704961528256832\n",
      "Epoch 36 completed\n",
      "Train Cost: 0.39502621313571135, Train Accuracy: 0.9189882373750774\n",
      "Validation Cost: 0.7772742823323674, Validation Accuracy: 0.767842929158928\n",
      "Epoch 38 completed\n",
      "Train Cost: 0.3849001141180501, Train Accuracy: 0.9212876978862652\n",
      "Validation Cost: 0.7766313724879313, Validation Accuracy: 0.767842929158928\n",
      "Epoch 40 completed\n",
      "Train Cost: 0.3754493713873698, Train Accuracy: 0.9234987176085611\n",
      "Validation Cost: 0.7762079475269595, Validation Accuracy: 0.766781639692226\n",
      "Epoch 42 completed\n",
      "Train Cost: 0.36658053059679846, Train Accuracy: 0.9260635004864244\n",
      "Validation Cost: 0.7761682556757367, Validation Accuracy: 0.7673122844255771\n",
      "Epoch 44 completed\n",
      "Train Cost: 0.35825000730526046, Train Accuracy: 0.9277438754753693\n",
      "Validation Cost: 0.7763531570307348, Validation Accuracy: 0.7683735738922791\n",
      "Epoch 46 completed\n",
      "Train Cost: 0.35040531970808025, Train Accuracy: 0.9294242504643141\n",
      "Validation Cost: 0.7766544613390822, Validation Accuracy: 0.7681082515256036\n",
      "Epoch 48 completed\n",
      "Train Cost: 0.3430040281676958, Train Accuracy: 0.9312815070310427\n",
      "Validation Cost: 0.7774133143234008, Validation Accuracy: 0.7681082515256036\n"
     ]
    }
   ],
   "source": [
    "weights = np.zeros((n_features, n_classes), dtype=np.float32)\n",
    "weights = gradient_descent(X_train, y_train_one_hot, X_val, y_val_one_hot, weights, learning_rate, num_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed\n",
      "Train Cost: 0.3361444309603628, Train Accuracy: 0.9334925267533386\n",
      "Validation Cost: 0.7780619729046099, Validation Accuracy: 0.7673122844255771\n",
      "Epoch 2 completed\n",
      "Train Cost: 0.32984901973164926, Train Accuracy: 0.934907579375608\n",
      "Validation Cost: 0.7792328782922795, Validation Accuracy: 0.766781639692226\n",
      "Epoch 4 completed\n",
      "Train Cost: 0.32385670888294177, Train Accuracy: 0.9368532767312284\n",
      "Validation Cost: 0.7803446249817895, Validation Accuracy: 0.765720350225524\n",
      "Epoch 6 completed\n",
      "Train Cost: 0.31814342684588204, Train Accuracy: 0.938179888564606\n",
      "Validation Cost: 0.7816772757326858, Validation Accuracy: 0.765189705492173\n",
      "Epoch 8 completed\n",
      "Train Cost: 0.31268303437777406, Train Accuracy: 0.9400371451313345\n",
      "Validation Cost: 0.782748562362166, Validation Accuracy: 0.7654550278588484\n",
      "Epoch 10 completed\n",
      "Train Cost: 0.3074688495778386, Train Accuracy: 0.9412753161758203\n",
      "Validation Cost: 0.7841076544986113, Validation Accuracy: 0.7665163173255506\n",
      "Epoch 12 completed\n",
      "Train Cost: 0.30247226924639814, Train Accuracy: 0.9427788095869816\n",
      "Validation Cost: 0.7856307539519184, Validation Accuracy: 0.765720350225524\n",
      "Epoch 14 completed\n",
      "Train Cost: 0.29768490446996193, Train Accuracy: 0.9438400990536836\n",
      "Validation Cost: 0.7870928184632946, Validation Accuracy: 0.7649243831254975\n",
      "Epoch 16 completed\n",
      "Train Cost: 0.2930847306885877, Train Accuracy: 0.9455204740426285\n",
      "Validation Cost: 0.7890187870107952, Validation Accuracy: 0.765720350225524\n",
      "Epoch 18 completed\n",
      "Train Cost: 0.2886657942599096, Train Accuracy: 0.9462280003537632\n",
      "Validation Cost: 0.7906456938250672, Validation Accuracy: 0.765189705492173\n",
      "Epoch 20 completed\n",
      "Train Cost: 0.28441804013413113, Train Accuracy: 0.9478199345538162\n",
      "Validation Cost: 0.7924790946846091, Validation Accuracy: 0.764659060758822\n",
      "Epoch 22 completed\n",
      "Train Cost: 0.28031376882811093, Train Accuracy: 0.9489696648094101\n",
      "Validation Cost: 0.794116610046886, Validation Accuracy: 0.765189705492173\n",
      "Epoch 24 completed\n",
      "Train Cost: 0.27636415355511224, Train Accuracy: 0.9500309542761122\n",
      "Validation Cost: 0.795939793939912, Validation Accuracy: 0.7649243831254975\n",
      "Epoch 26 completed\n",
      "Train Cost: 0.27254899873523347, Train Accuracy: 0.9513575661094897\n",
      "Validation Cost: 0.7978169005768535, Validation Accuracy: 0.7654550278588484\n",
      "Epoch 28 completed\n",
      "Train Cost: 0.2688698614667679, Train Accuracy: 0.9526841779428673\n",
      "Validation Cost: 0.7997389388433884, Validation Accuracy: 0.765720350225524\n",
      "Epoch 30 completed\n",
      "Train Cost: 0.2653048069523158, Train Accuracy: 0.953391704254002\n",
      "Validation Cost: 0.8016759610020703, Validation Accuracy: 0.765189705492173\n",
      "Epoch 32 completed\n",
      "Train Cost: 0.2618570552479664, Train Accuracy: 0.9546298752984876\n",
      "Validation Cost: 0.8037548252541239, Validation Accuracy: 0.7635977712921199\n",
      "Epoch 34 completed\n",
      "Train Cost: 0.2585159141358623, Train Accuracy: 0.9554258423985142\n",
      "Validation Cost: 0.8054617557226632, Validation Accuracy: 0.7638630936587955\n",
      "Epoch 36 completed\n",
      "Train Cost: 0.25528195812023996, Train Accuracy: 0.9563102502874326\n",
      "Validation Cost: 0.807430241411862, Validation Accuracy: 0.7633324489254444\n",
      "Epoch 38 completed\n",
      "Train Cost: 0.25215154895239666, Train Accuracy: 0.9569293358096754\n",
      "Validation Cost: 0.8094451361748208, Validation Accuracy: 0.7620058370920668\n",
      "Epoch 40 completed\n",
      "Train Cost: 0.24910729965008346, Train Accuracy: 0.9576368621208101\n",
      "Validation Cost: 0.8114911127368991, Validation Accuracy: 0.7625364818254179\n",
      "Epoch 42 completed\n",
      "Train Cost: 0.24615359210356316, Train Accuracy: 0.9584328292208366\n",
      "Validation Cost: 0.8135817043494763, Validation Accuracy: 0.7606792252586894\n",
      "Epoch 44 completed\n",
      "Train Cost: 0.24327674459172785, Train Accuracy: 0.9588750331652959\n",
      "Validation Cost: 0.8154838728637402, Validation Accuracy: 0.7614751923587159\n",
      "Epoch 46 completed\n",
      "Train Cost: 0.2404889294687373, Train Accuracy: 0.9603785265764571\n",
      "Validation Cost: 0.8176616805890342, Validation Accuracy: 0.7609445476253648\n",
      "Epoch 48 completed\n",
      "Train Cost: 0.23777131215469083, Train Accuracy: 0.9608207305209162\n",
      "Validation Cost: 0.819684595145151, Validation Accuracy: 0.7604139028920138\n"
     ]
    }
   ],
   "source": [
    "weights_bcgd = np.zeros((n_features, n_classes), dtype=np.float32)\n",
    "weights_bcgd = block_coordinate_gradient_descent(X_train, y_train_one_hot, X_val, y_val_one_hot, weights, learning_rate, num_epochs, batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with classic gradient descent: 0.05075670693524897\n",
      "Accuracy with block coordinate gradient descent: 0.05075670693524897\n"
     ]
    }
   ],
   "source": [
    "y_test = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "y_pred = predict(X_test, weights)\n",
    "if len(y_pred) != y_test_one_hot.shape[0]:\n",
    "    raise ValueError(f'Shape mismatch: y_pred has shape {y_pred.shape}, y_test has shape {y_test_one_hot.shape[0]}')\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Accuracy with classic gradient descent: {accuracy}')\n",
    "\n",
    "# Predict using block coordinate gradient descent weights\n",
    "y_pred_bcgd = predict(X_test, weights_bcgd)\n",
    "if len(y_pred_bcgd) != y_test_one_hot.shape[0]:\n",
    "    raise ValueError(f'Shape mismatch: y_pred_bcgd has shape {y_pred_bcgd.shape}, y_test has shape {y_test_one_hot.shape[0]}')\n",
    "accuracy_bcgd = np.mean(y_pred_bcgd == y_test)\n",
    "print(f'Accuracy with block coordinate gradient descent: {accuracy_bcgd}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
